{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Fraud from Enron Email\n",
    "Project for Intro to Machine Learning\n",
    "## Goal:\n",
    "\n",
    "### Project Overview\n",
    "In 2000, Enron was one of the largest companies in the United States. By 2002, it had collapsed into bankruptcy due to widespread corporate fraud. In the resulting Federal investigation, a significant amount of typically confidential information entered into the public record, including tens of thousands of emails and detailed financial data for top executives. In this project, you will play detective, and put your new skills to use by building a person of interest identifier based on financial and email data made public as a result of the Enron scandal. To assist you in your detective work, we've combined this data with a hand-generated list of persons of interest in the fraud case, which means individuals who were indicted, reached a settlement or plea deal with the government, or testified in exchange for prosecution immunity.\n",
    "\n",
    "Provided is a combined the Enron email and financial dataset in the form of a dictionary, where each key-value pair in the dictionary corresponds to one person. The dictionary key is the person's name, and the value is another dictionary, which contains the names of all the features and their values for that person. The features in the data fall into three major types, namely financial features, email features and POI labels.\n",
    "\n",
    "### Project goal\n",
    "The goal of the project is to see if I an make building a person of interest identifier that has reasonable accuracy and other metric scores to identify persons of interest from based on financial and email data made public as a result of the Enron scandal.\n",
    "\n",
    "### Project Plan\n",
    "The steps to achieve this will be:\n",
    "\n",
    "* Dataset exploration / Question\n",
    "* Features\n",
    "* Algorithms\n",
    "* Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset exploration / Question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In the dataset there is a mix of different data: payment, stock and email information\n",
    "\n",
    "payment features:  \n",
    "\n",
    "| Feature | meaning |\n",
    "|: -------|:--------|\n",
    "| salary | Reflects items such as base salary, executive cash allowances, and benefits payments |\n",
    "| bonus | Reflects annual cash incentives paid based upon company performance. Also may include other retention payments. |\n",
    "| long_term_incentive |Reflects long-term incentive cash payments from various long-term incentive programs designed to tie executive compensation to long-term success as measured against key performance drivers and business objectives over a multi-year period, generally 3 to 5 years. |\n",
    "| deferred_income | Reflects voluntary executive deferrals of salary, annual cash incentives, and long-term cash incentives as well as cash fees deferred by non-employee directors under a deferred compensation arrangement. May also reflect deferrals under a stock option or phantom stock unit in lieu of cash arrangement.  |\n",
    "| deferral_payments  |Reflects distributions from a deferred compensation arrangement due to termination of employment or due to in-service withdrawals as per plan provisions. |\n",
    "| loan_advances  | Reflects total amount of loan advances, excluding repayments, provided by the Debtor in return for a promise of repayment. In certain instances, the terms of the promissory notes allow for the option to repay with stock of the company. |\n",
    "| other  |Reflects items such as payments for severence, consulting services, relocation costs, tax advances and allowances for employees on international assignment (i.e. housing allowances, cost of living allowances, payments under Enronâ€™s Tax Equalization Program, etc.). May also include payments provided with respect to employment agreements, as well as imputed income amounts for such things as use of corporate aircraft. |\n",
    "| expenses  | Reflects reimbursements of business expenses. May include fees paid for consulting services |\n",
    "| director_fees | Reflects cash payments and/or value of stock grants made in lieu of cash payments to non-employee directors. | \n",
    "| total_payments  | cumulative of all payments |\n",
    "  \n",
    "Stock features  \n",
    "\n",
    "| Feature | meaning |\n",
    "|: -------|:--------|\n",
    "| exercised_stock_options | Reflects amounts from exercised stock options which equal the market value in excess of the exercise price on the date the options were exercised either through cashless (same-day sale), stock swap or cash exercises. The reflected gain may differ from that realized by the insider due to fluctuations in the market price and the timing of any subsequent sale of the securities. |\n",
    "| restricted_stock | Reflects the gross fair market value of shares and accrued dividends (and/or phantom units and dividend equivalents) on the date of release due to lapse of vesting periods, regardless of whether deferred. |\n",
    "| restricted_stock_deferred  | Reflects value of restricted stock voluntarily deferred prior to release under a deferred compensation arrangement. |\n",
    "| total_stock_value | cumulative value of stock |\n",
    "  \n",
    "email features  \n",
    "  \n",
    "| Feature | meaning |\n",
    "|: -------|:--------|\n",
    "| from_poi_to_this_person  | emails send from POI to this person |\n",
    "| shared_receipt_with_poi  | emails which were also send to a POI |\n",
    "| from_this_person_to_poi  | emails sent from this person to POI |\n",
    "| to_messages | total messages to this person |\n",
    "| from_messages | total messages from this person |\n",
    "  \n",
    "and then of course if the person is a poi, in this case identified as either sentenced or someone who took a plea deal as noted in this article : http://usatoday30.usatoday.com/money/industries/energy/2005-12-28-enron-participants_x.htm and provided in the poi_names.txt\n",
    "\n",
    "| Feature | meaning |\n",
    "|---------|---------|\n",
    "| poi  | person of interest |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and outliers\n",
    "The dataset had some small problems:\n",
    "* **non-persons in the data** (Total row was added as data point, and a travel agency as well), these were removed\n",
    "* **Person with no data** was removed\n",
    "* **two people with shifted columns** where the data was manually corrected\n",
    "\n",
    "Outside these problems, all outliers are an important part of the dataset, since the biggest outliers are the biggest POIs\n",
    "* total number of data points\n",
    "* allocation across classes (POI/non-POI)\n",
    "* number of features available\n",
    "* are there features with many missing values? etc.\n",
    "\n",
    "There are 18 poi identified in the dataset  \n",
    "There are 125 non-poi identified in the dataset  \n",
    "  \n",
    "number of features available : 20, total number of datapoints: 143  \n",
    "  \n",
    "Feature : poi, known values : 18  \n",
    "Feature : salary, known values : 94  \n",
    "Feature : bonus, known values : 81  \n",
    "Feature : long_term_incentive, known values : 65  \n",
    "Feature : deferred_income, known values : 49  \n",
    "Feature : deferral_payments, known values : 37  \n",
    "Feature : loan_advances, known values : 3  \n",
    "Feature : other, known values : 90  \n",
    "Feature : expenses, known values : 96  \n",
    "Feature : director_fees, known values : 15  \n",
    "Feature : total_payments, known values : 123  \n",
    "Feature : exercised_stock_options, known values : 100  \n",
    "Feature : restricted_stock, known values : 110  \n",
    "Feature : restricted_stock_deferred, known values : 17  \n",
    "Feature : total_stock_value, known values : 125  \n",
    "Feature : from_poi_to_this_person, known values : 74  \n",
    "Feature : shared_receipt_with_poi, known values : 86  \n",
    "Feature : from_this_person_to_poi, known values : 66  \n",
    "Feature : to_messages, known values : 86  \n",
    "Feature : from_messages, known values : 86  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection/Engineering\n",
    "Looking at the original features in the dataset, email-addresses were removed, since they do not add information regarding poi or not.\n",
    "\n",
    "To create more meaningfull features, I've introduced features relative to the total (salary / total payment for instance). I've used SelectPercentile from sklearn as a metric to evaluate if the relative features score better than the original metric.\n",
    "\n",
    "I've then removed the original features and kept the relative ones:\n",
    "\n",
    "<code> features_list = ['poi',\n",
    "                 'from_messages',\n",
    "                 'restricted_stock_deferred',\n",
    "                 'to_messages',\n",
    "                 'director_fees',\n",
    "                 'other',\n",
    "                 'from_poi_to_this_person',\n",
    "                 'expenses',\n",
    "                 'loan_advances',\n",
    "                 'restricted_stock',\n",
    "                 'total_payments',\n",
    "                 'deferred_income',\n",
    "                 'salary',\n",
    "                 'bonus',\n",
    "                 'exercised_stock_options',\n",
    "                 'total_stock_value',\n",
    "                 'deferral_payments_r',\n",
    "                 'shared_receipt_with_poi_r',\n",
    "                 'long_term_incentive_r',\n",
    "                 'from_this_person_to_poi_r']</code>\n",
    "    \n",
    "Where the features denoted with _r suffix are features relative to the total. (ie. long_term_incentive_r = long_term_incentive / total_payments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick and Tune an Algorithm\n",
    "\n",
    "To determine which algoritm performs best, I created a for loop that checks the algorithms with gridSearhCV for al the selected classifiers methods. For the parameters to be tuned, I've tried to keep some continuity between the algorithms between the parameter grids.\n",
    "\n",
    "I've selected the following classifiers:\n",
    "* SVC\n",
    "* DecisionTreeClassifier\n",
    "* RandomForestClassifier\n",
    "* AdaBoostClassifier\n",
    "* GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_test_set</th>\n",
       "      <th>accuracy_training_set</th>\n",
       "      <th>best_params</th>\n",
       "      <th>classifier</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>scaling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.925161</td>\n",
       "      <td>0.931852</td>\n",
       "      <td>{u'kernel': u'poly', u'C': 50}</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Data after quantile transformation (uniform pdf)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.907197</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{u'max_features': u'sqrt', u'loss': u'deviance...</td>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Data after min-max scaling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.890645</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{u'min_samples_split': 2, u'splitter': u'rando...</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Data after quantile transformation (gaussian pdf)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.853078</td>\n",
       "      <td>0.989636</td>\n",
       "      <td>{u'min_samples_split': 8, u'splitter': u'best'...</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.6</td>\n",
       "      <td>Data after min-max scaling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.853078</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>{u'min_samples_split': 2, u'splitter': u'best'...</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.6</td>\n",
       "      <td>Data after max-abs scaling</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy_test_set  accuracy_training_set  \\\n",
       "25           0.925161               0.931852   \n",
       "9            0.907197               1.000000   \n",
       "21           0.890645               1.000000   \n",
       "6            0.853078               0.989636   \n",
       "11           0.853078               1.000000   \n",
       "\n",
       "                                          best_params  \\\n",
       "25                     {u'kernel': u'poly', u'C': 50}   \n",
       "9   {u'max_features': u'sqrt', u'loss': u'deviance...   \n",
       "21  {u'min_samples_split': 2, u'splitter': u'rando...   \n",
       "6   {u'min_samples_split': 8, u'splitter': u'best'...   \n",
       "11  {u'min_samples_split': 2, u'splitter': u'best'...   \n",
       "\n",
       "                    classifier  f1_score  precision_score  recall_score  \\\n",
       "25                         SVC  0.571429         1.000000           0.4   \n",
       "9   GradientBoostingClassifier  0.500000         0.666667           0.4   \n",
       "21      DecisionTreeClassifier  0.444444         0.500000           0.4   \n",
       "6       DecisionTreeClassifier  0.428571         0.333333           0.6   \n",
       "11      DecisionTreeClassifier  0.428571         0.333333           0.6   \n",
       "\n",
       "                                              scaling  \n",
       "25   Data after quantile transformation (uniform pdf)  \n",
       "9                          Data after min-max scaling  \n",
       "21  Data after quantile transformation (gaussian pdf)  \n",
       "6                          Data after min-max scaling  \n",
       "11                         Data after max-abs scaling  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.nlargest(5, 'f1_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of these, the SVC performed best based:\n",
    "* **F1 score**, to make sure we do not have a high inbalance in the results, we use f1 scaling to check the quality of the algorithm  \n",
    "In addition to that, it also:\n",
    "* **High precision and a more modest recall**  which means that all of the poi's identified are indeed poi's, but it misses quite some (false negatives) because it is so picky. I think for this purpose it is quite a good thing, since we want to be as sure as we can before accusing someone, as opposed to for instance a medical application where a  false positive would be more acceptable if it means more coverage. \n",
    "* **Accuracy for the test set which is not far from the accuracy of the training set** which means it is not overfitting the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "Implementing all these in the [*poi_id.py*](poi_id.py), and testing it with the tester.py shows the message:\n",
    "<code>Pipeline(memory=None,\n",
    "     steps=[('scaler', QuantileTransformer(copy=True, ignore_implicit_zeros=False, n_quantiles=1000,\n",
    "          output_distribution='uniform', random_state=None,\n",
    "          subsample=100000)), ('SVC', SVC(C=50, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
    "  kernel='poly', max_iter=-1, probability=False, random_state=None,\n",
    "  shrinking=True, tol=0.001, verbose=False))])\n",
    "        Accuracy: 0.90480       Precision: 0.77186      Recall: 0.40600 F1: 0.53211     F2: 0.44852\n",
    "        Total predictions: 15000        True positives:  812    False positives:  240   False negatives: 1188   True negatives: 12760</code>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have:\n",
    "* Total predictions: 15000        \n",
    "* True positives:  812    \n",
    "* False positives:  240   \n",
    "* False negatives: 1188   \n",
    "* True negatives: 12760\n",
    "\n",
    "\n",
    "* Accuracy: 0.90480       \n",
    "* Precision: 0.77186      \n",
    "* Recall: 0.40600 \n",
    "* F1: 0.53211\n",
    "* F2: 0.44852  \n",
    "  \n",
    "  \n",
    "\n",
    "So it seems our classifier is doing a pretty good job: \n",
    "* 90% of the cases are predicted correctly (either positive or negative) (accuracy)\n",
    "* with a 77% Precision, the TP/(TP+FP) is bigger then the recall, which is what we wanted, but both of them are high enough to maintain good quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
